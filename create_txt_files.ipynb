{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe containing the ID of the images in train01\n",
    "images_in_train01 = glob.glob(\"Datasets\\\\CrowdHuman\\\\CrowdHuman_train01 (1)\\\\Images\\\\*\")\n",
    "images_in_train01 = [i.replace('Datasets\\\\CrowdHuman\\\\CrowdHuman_train01 (1)\\\\Images\\\\', '') for i in images_in_train01]\n",
    "# images_in_train01 = glob.glob(\"Datasets\\\\CrowdHuman\\\\CrowdHuman_train01\\\\Images\\\\*\")\n",
    "# images_in_train01 = [i.replace('Datasets\\\\CrowdHuman\\\\CrowdHuman_train01\\\\Images\\\\', '') for i in images_in_train01]\n",
    "images_in_train01 = pd.DataFrame({'ID2': images_in_train01})\n",
    "\n",
    "# Dataframe containing the ID of the images in the subset of train01 (1000 images)\n",
    "images_in_train_subset = glob.glob(\"Datasets\\\\CrowdHuman\\\\CrowdHuman_train_subset_1000\\\\Images\\\\*\")\n",
    "images_in_train_subset = [i.replace('Datasets\\\\CrowdHuman\\\\CrowdHuman_train_subset_1000\\\\Images\\\\', '') for i in images_in_train_subset]\n",
    "images_in_train_subset = pd.DataFrame({'ID2': images_in_train_subset})\n",
    "\n",
    "# Dataframe containing the ID of the images in the subset of val (600 images)\n",
    "images_in_val_subset = glob.glob(\"Datasets\\\\CrowdHuman\\\\CrowdHuman_val_subset_600\\\\Images\\\\*\")\n",
    "images_in_val_subset = [i.replace('Datasets\\\\CrowdHuman\\\\CrowdHuman_val_subset_600\\\\Images\\\\', '') for i in images_in_val_subset]\n",
    "images_in_val_subset = pd.DataFrame({'ID2': images_in_val_subset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open odgt files\n",
    "with open('Datasets/CrowdHuman/annotation_train.odgt') as f:\n",
    "    lines_train = f.readlines()\n",
    "with open('Datasets/CrowdHuman/annotation_val.odgt') as f:\n",
    "    lines_val = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odgt_to_dataframe(line, images_path):\n",
    "    '''\n",
    "    This function extracts for every line of the odgt file the ID of the image, the the tags ('person' or 'mask'), and the full bounding box related to each of the tags.\n",
    "    \n",
    "    Input: a line from the odgt file\n",
    "    Output: a dataframe  \n",
    "    '''\n",
    "    # Extract tag and ID\n",
    "    df = pd.json_normalize(json.loads(line)['gtboxes'])\n",
    "    df['ID'] = images_path+json.loads(line)['ID']+'.jpg'\n",
    "    df['ID2'] = json.loads(line)['ID']+'.jpg'\n",
    "\n",
    "    # convert from x,y,w,h to x1,y1,x2,y2\n",
    "    df1 = pd.DataFrame(df['fbox'].to_list(), columns = ['x1', 'y1', 'w', 'h'])\n",
    "    df1['x2'] = df1['x1'] + df1['w'] \n",
    "    df1['y2'] = df1['y1'] + df1['h']\n",
    "    df1 = df1.drop(columns=['w','h'])\n",
    "\n",
    "    # Putting coordinates, tags and ID together\n",
    "    df1['tag'] = df['tag']\n",
    "    df1['ID'] = df['ID']\n",
    "    df1['ID2'] = df['ID2']\n",
    "    \n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109188, 6)\n",
      "(116606, 6)\n",
      "(22166, 6)\n",
      "(12957, 6)\n"
     ]
    }
   ],
   "source": [
    "# Apply odgt_to_dataframe to each line and save all results in a dataset \n",
    "\n",
    "# Training set\n",
    "\n",
    "train_path = 'Datasets/CrowdHuman/CrowdHuman_train01/Images/'\n",
    "train0 = pd.concat([odgt_to_dataframe(line = l, images_path = train_path) for l in lines_train])\n",
    "# keep only the information from images in train01\n",
    "train = pd.merge(train0,images_in_train01,on='ID2') \n",
    "first_column = train.pop('ID')\n",
    "train.insert(0, 'ID', first_column)\n",
    "train = train.replace({'tag': 'mask'}, 'ignore')\n",
    "# keep only information from images that have the ignore category\n",
    "train_with_ignore = pd.DataFrame({'ID2': np.unique(train['ID2'][train['tag'] == 'ignore'])})\n",
    "train = pd.merge(train,train_with_ignore,on='ID2')\n",
    "train = train.drop(columns=['ID2'])\n",
    "print(train.shape)\n",
    "\n",
    "# Validation set\n",
    "\n",
    "val_path = 'Datasets/CrowdHuman/CrowdHuman_val/Images/'\n",
    "val = pd.concat([odgt_to_dataframe(line = l, images_path = val_path) for l in lines_val])\n",
    "first_column = val.pop('ID')\n",
    "val.insert(0, 'ID', first_column)\n",
    "val = val.replace({'tag': 'mask'}, 'ignore')\n",
    "# keep only information from images that have the ignore category\n",
    "val_with_ignore = pd.DataFrame({'ID2': np.unique(val['ID2'][val['tag'] == 'ignore'])})\n",
    "val = pd.merge(val,val_with_ignore,on='ID2')\n",
    "val = val.drop(columns=['ID2'])\n",
    "print(val.shape)\n",
    "\n",
    "# Training set subset of 1000 images\n",
    "\n",
    "train_subset_path = 'Datasets/CrowdHuman/CrowdHuman_train_subset_1000/Images/'\n",
    "train_subset0 = pd.concat([odgt_to_dataframe(line = l, images_path = train_subset_path) for l in lines_train])\n",
    "# keep only the information from images in train subset\n",
    "train_subset = pd.merge(train_subset0,images_in_train_subset,on='ID2') \n",
    "first_column = train_subset.pop('ID')\n",
    "train_subset.insert(0, 'ID', first_column)\n",
    "train_subset = train_subset.replace({'tag': 'mask'}, 'ignore')\n",
    "# keep only information from images that have the ignore category\n",
    "train_subset_with_ignore = pd.DataFrame({'ID2': np.unique(train_subset['ID2'][train_subset['tag'] == 'ignore'])})\n",
    "train_subset = pd.merge(train_subset,train_subset_with_ignore,on='ID2')\n",
    "train_subset = train_subset.drop(columns=['ID2'])\n",
    "print(train_subset.shape)\n",
    "\n",
    "# Validation set subset of 600 images\n",
    "\n",
    "val_subset_path = 'Datasets/CrowdHuman/CrowdHuman_val_subset_600/Images/'\n",
    "val_subset0 = pd.concat([odgt_to_dataframe(line = l, images_path = val_subset_path) for l in lines_val])\n",
    "# keep only the information from images in train subset\n",
    "val_subset = pd.merge(val_subset0,images_in_val_subset,on='ID2') \n",
    "first_column = val_subset.pop('ID')\n",
    "val_subset.insert(0, 'ID', first_column)\n",
    "val_subset = val_subset.replace({'tag': 'mask'}, 'ignore')\n",
    "# keep only information from images that have the ignore category\n",
    "val_subset_with_ignore = pd.DataFrame({'ID2': np.unique(val_subset['ID2'][val_subset['tag'] == 'ignore'])})\n",
    "val_subset = pd.merge(val_subset,val_subset_with_ignore,on='ID2')\n",
    "val_subset = val_subset.drop(columns=['ID2'])\n",
    "print(val_subset.shape)\n",
    "\n",
    "# Sample of 5 images from validation\n",
    "\n",
    "val_small_sample = pd.merge(val_subset,pd.DataFrame({'ID':np.unique(val_subset['ID'])[0:4]}), on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the classes (person and mask)\n",
    "classes = np.unique(train['tag'])\n",
    "classes = pd.DataFrame({'a':classes, 'b': range(len(classes))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in txt\n",
    "train.to_csv('Datasets/CrowdHuman/train.txt', header=None, index=None, sep=' ')\n",
    "val.to_csv('Datasets/CrowdHuman/val.txt', header=None, index=None, sep=' ')\n",
    "\n",
    "train_subset.to_csv('Datasets/CrowdHuman/train_subset.txt', header=None, index=None, sep=' ')\n",
    "val_subset.to_csv('Datasets/CrowdHuman/val_subset.txt', header=None, index=None, sep=' ')\n",
    "\n",
    "val_small_sample.to_csv('Datasets/CrowdHuman/val_small_sample.txt', header=None, index=None, sep=' ')\n",
    "\n",
    "classes.to_csv('Datasets/CrowdHuman/classes.txt', header=None, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d97ad4eda96f4e0dcd5ae4f97368654619500468c6147550d2a1b2a1881f9a5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
